---
title: "Modelos de riesgo de crédito con Redes Neuronales Artificiales"
format: 
  html:
    fig-width: 8
    fig-height: 6
    number-sections: true
    math: 
      method: mathjax
      options:
        TeX:
          equationNumbers: { autoNumber: "AMS" }
author:
  - name: "Julián Castaño Pineda"
  - name: "Luis Andrés Altamar Romero"
  - name: "Catalina Restrepo Salgado"
  - name: "Tomás Rodríguez Taborda"
date: "2025-01-23"
categories: [redes neuronales artificiales, desarrollo web, python]
image: "image.jpg"
bibliography: ref.bib
execute:
  cache: true
---

# Introducción a la evaluación y gestión del riesgo de crédito

El riesgo de crédito, de acuerdo con información encontrada en el sitio web @financionario2025 y, en concordancia con lo dicho por @financestrategists2023, se define como una medida empleada para dimensionar el riesgo (en términos de probabilidad) de que un prestatario incumpla con el pago de una obligación financiera o el reembolso del dinero correspondiente a un préstamo.

En este sentido, es posible notar que una correcta evaluación y gestión del riesgo de crédito es de vital importancia en las actividades relacionadas con el préstamo y la inversión, pues ayudan a que las entidades financieras puedan mantener su estabilidad, a la misma vez que se proyectan como instituciones de confianza ante las entidades estatales, sus socios y clientes. Adicionalmente, el riesgo de crédito suele tenerse en cuenta como un criterio a la hora de aprobar o definir las condiciones de un préstamo, solicitando garantías de respaldo al prestatario o ajustando la tasa de interés de acuerdo con estos resultados.

Sabiendo lo anterior, puede decirse que, tanto para las instituciones financieras como para quienes solicitan estos servicios, el riesgo de crédito es un factor de incertidumbre que influye en la toma de decisiones relacionada con la admisión de préstamos y otros productos financieros, entendiendo incertidumbre como "la falta de (...) certeza o de un conocimiento seguro respecto de una determinada situación", según @significadode2023.

![Adaptado de Ilustración del plan de ahorro de los empleados dibujada a mano\[Ilustración\], por Freepik, 2024 (https://www.freepik.es/vector-gratis/ilustracion-plan-ahorro-empleados-dibujada-mano_87161866.htm#fromView=search&page=1&position=11&uuid=e54defbf-b176-455c-82c3-276ae1b3c634&new_detail=true). Licencia gratuita.](imagen_credito.jpg)

## Delimitaciones del problema y metodología

Considerando entonces la importancia del estudio del riesgo de crédito y sus implicaciones en las decisiones que toman las instituciones financieran que invierten y otorgan préstamos, así como las consecuencias que estas decisiones acarrean sobre las personas que los solicitas, se decide abordar esta cuestión a partir desde el punto de vista de los conocimientos adquiridos en el curso. Esto, pues la evaluación del riesgo de crédito involucra tareas como las que se enuncian a continuación:

-   **Manejo de grandes volúmenes de datos:** las entidades financieras o estatales poseen un amplio registro de información sobre los clientes que acceden a estos servicios, tales como sus comportamientos de pago y datos demográficos.

-   **Identificación de patrones no evidentes:** la probabilidad de incumplimiento de un cliente con sus pagos no siempre tendrá una relación lineal con las características que se conocen acerca de su persona, por lo que pueden necesitarse técnicas de mayor complejidad para descubrir los patrones o relaciones que lo explican.

-   **Toma de decisiones basadas en riesgo**: correspondiente al objetivo final de la presente actividad, se es importante realizar una clasificación de los clientes según su nivel de riesgo de crédito, asignando un puntaje que permita evaluar cualitativa y cuantitativamente las solicitudes de préstamo.

Dicho esto, puede verse que la elaboración de una herramienta que permita una evaluación adecuada del riesgo de crédito sería altamente beneficiosa no solo para las entidades financieras, que contarían con un medio de conocer mejor a sus posibles clientes, sino también para estos últimos, pues tendrían la oportunidad de conocer con antelación las posibilidades de que su crédito sea aprobado así como las variables que influirían en esa decisión.

Para tal fin se desarrollará una aplicación web que permitirá responder a la pregunta: *¿cuál es el puntaje de riesgo de crédito de un posible prestatario?* a través de la utilización de un modelo de redes neuronales artificiales entrenado sobre un amplio conjunto de datos en este contexto. El dataset a emplear, titulado **Credit Risk Analysis** y proporcionado por @r_g__2021 será tratado con la siguiente metodología y posteriormente evaluado, como podrá verse más adelante.

**Metodología**

1.  Análisis descriptivo e hipótesis del conjunto de datos.
2.  Planteamiento y evaluación de modelos.
3.  Conclusiones y aprendizajes a partir del modelo.
4.  Planteamiento de un caso de uso.

A continuación se da inicio al desarrollo de la actividad, la cual será complementada con todos los recursos empleados y productos resultantes de dicho ejercicio.

## Análisis descriptivo e hipótesis del conjunto de datos

## Planteamiento y evaluación de modelos

### Modelo de baja complejidad

La regresión logística es un algoritmo que a pesar de tener la palabra regresión en su nombre, esta pensado para la clasificación binaria, es posible además extender para problemas multiclase a través de la regresión logística multinomial. La salida que se obtiene de este algoritmo es un número entre 0 y 1 que representa la probabilidad de pertenecer a una clase, la suma de la probabilidad de todas las clases debe dar 1, y la clase con la mayor probabilidad es aquella donde la observación será clasificada.

Las probabilidades son calculadas de la siguiente manera:

$$
P(y = c \mid X) = \frac{e^{\beta_{c0} + \beta_{c1}X_1 + \dots + \beta_{cn}X_n}}{\sum_{k=1}^{C} e^{\beta_{k0} + \beta_{k1}X_1 + \dots + \beta_{kn}X_n}}  \tag{1}
$$

La expresión anterior es además posible de simplificar para introducir la función softmax la cual también será usada en la sección de redes neuronales, para ello escribimos la combinación lineal de las variables $X$ de la siguiente manera:

$$
z_c = \beta_{c0} + \beta_{c1}X_1 + \dots + \beta_{cn}X_n  \tag{2}
$$

Con lo cual la ecuación 1 queda de la siguiente manera:

$$
P(y = c \mid X)=\frac{e^{z_c}}{\sum_{k=1}^{C} e^{z_k}} \tag{3}
$$

La clase a la que pertenecerá la observación según el modelo de regresión logística multinomial esta dado por:

$$
\hat{y} = \arg\max_c \, P(y = c \mid X) \tag{4}
$$

A diferencia de otros algoritmos de aprendizaje automático, no cuenta con la misma cantidad de hiperparámetros que algoritmos como Random Forest o Support Vector Machines. La librería de Scikit-Learn permite ajustar el método y la fuerza de la regularización.

*Tipos de regularización*

-   **L1 (Lasso)**: Penaliza la suma de los valores absolutos de los coeficientes:

    $$
    \|\beta\|_1 = \sum_{i=1}^n |\beta_i| \tag{5}
    $$

    Esto lleva algunos coeficientes a cero, permitiendo la selección de características.

-   **L2 (Ridge)**: Penaliza la suma de los cuadrados de los coeficientes:

    $$
    \|\beta\|_2^2 = \sum_{i=1}^n \beta_i^2 \tag{6}
    $$

    Esto evita que los coeficientes crezcan demasiado grandes, ayudando a reducir el sobreajuste.

*Fuerza de la regularización*

Controla la fuerza de la regularización de forma inversamente proporcional

$$
C = \frac{1}{\lambda} \tag{7}
$$

-   Un valor grande de $C$ implica **menos regularización** (modelo más flexible).

-   Un valor pequeño de $C$ implica **más regularización** (modelo más restringido).

**Ventajas**

El modelo de regresión logístico presenta las siguientes ventajas:

1.  Interpretabilidad: Los coeficientes $\beta$ permiten una interpretación clara del impacto de cada variable sobre la probabilidad de pertenecer a una clase en específico.

2.  Fácil implementación: No requiere ajustes avanzados para su implementación.

3.  Grados de certeza: Debido a que proporciona las probabilidades de pertenecer a cada clase, permite evaluar el grado de certeza sobre la pertenencia del dato a cada una de ellas.

**Desventajas**

1.  Relaciones no lineales: La regresión logística esta pensada para relaciones lineales, por lo que si se buscan modelar relaciones no lineales requiere la introducción de términos polinómicos.

2.   Independencia de alternativas irrelevantes: El modelo asume que la relación entre dos clases no cambia si se introduce una nueva clase, lo que no siempre es realista.

### Modelo de Redes Neuronales Artificiales

Las redes neuronales artificiales son modelos computacionales agrupados dentro del *machine learning*, con los cuales se busca simular el comportamiento del cerebro humano, de acuerdo con @unirrna2021 y @ibmrna2025. Su funcionamiento imita el procedimiento con el que trabajan en conjunto las neuronas biológicas para aprender de la información que reciben desde el exterior o por parte de otras neuronas.

Gracias a su comportamiento, basado en la utilización de datos de entrenamiento para mejorar paulatinamente la precisión de sus resultados, las redes neuronales permiten la realización de tareas que no se podían automatizar en otro tipo de modelos, lo que ha llevado al desarrollo de avances significativos en el área de la inteligencia artificial e impactando de forma directa a las personas e industrias.

Considerando lo dicho anteriormente, es posible ver que las redes neuronales artificiales son una gran alternativa a la hora de enfrentar tareas relacionadas con el aprendizaje a partir de un conjunto de datos, encontrando patrones que puedan explicar el comportamiento de estos últimos respecto de alguna variable objetivo, con la finalidad de tomar una decisión después del entendimiento de dicho fenómeno. En este sentido, será utilizada una red neuronal para mejorar el resultado obtenido con el modelo de baja complejidad en el presente ejercicio.

#### Teorema de Aproximación Universal

Este teorema establece que:

"Cualquier función continua definida en un conjunto compacto puede ser aproximada arbitrariamente bien por una red neuronal feedforward con una sola capa oculta y un número suficiente de neuronas, utilizando funciones de activación no lineales"

Lo anterior significa que las funciones de activación no lineales son esenciales para que las redes neuronales puedan aproximar funciones complejas. Si bien el teorema garantiza que una sola capa oculta con suficientes neuronas puede aproximar cualquier función continua en un conjunto compacto, en la práctica, aumentar el número de capas suele ser más eficiente para modelar problemas complejos.

![Ilustración de los resultados obtenidos mediantes funciones no lineales comparadas con las funciones lineales.](no_linealidad.png)

#### Arquitectura de una red neuronal artificial

Las redes neuronales están compuestas por un conjunto de nodos, que vendrían siendo las neuronas artificiales, repartidos en capas que pertenecen a alguna de las siguientes tres categorías, con información obtenida de @cloudflare y @paroledevs:

-   **Capa de entrada:** esta capa tiene conexión con el "mundo exterior" a la red neuronal artificial, recibiendo los datos iniciales que serán procesados por ella.

-   **Capa de salida:** esta capa proporciona el resultado del procesamiento realizado por la red neuronal, comúnmente como una predicción o una clasificación (lo cual se ve internamente en términos de probabilidad).

-   **Capas ocultas:** una o más capas que se encuentran entre las dos mencionadas anteriormente, realizando el procesamiento y extracción de características de los datos. Este análisis se realiza de menor a mayor profundidad, pues cada capa extrae los patrones más significativos de los datos que recibió como entrada y los envía a una capa superior para que sean vistos con más detalle.

Ahora bien, es conveniente conocer los elementos que componen a cada uno de los nodos que interactúan en las capas anteriormente mencionadas, pues de esta manera será posible comprender la importancia del correcto diseño de la arquitectura de una red neuronal artificial. De acuerdo con @franciscopalaciorna, estos elementos son:

**Entrada:** los datos que recibe la neurona artificial del exterior o de otras neuronas, se representa como un vector $x = (x_1, x_2, ..., x_n)$.

**Pesos sinápticos:** representan los factores de importancia $w_{ij}$ que se le asignan a las entradas que cada neurona recibió de su anterior compañera. Son valores numéricos que se modifican durante el entrenamiento del modelo y poseen una vital importancia en el desempeño de este mismo frente al conjunto de datos del que está aprendiendo.

**Regla de propagación:** una operación que se aplica de forma primordial a los datos de entrada y los pesos para calcular el posible valor de la salida de la neurona artificial; generalmente es una suma ponderada pero también pueden ser otras clases de operaciones.

**Función de activación capa de entrada:** el valor obtenido con la regla de propagación se procesa con a través de esta función, a fin de obtener el verdadero resultado de salida de la neurona. Existe una gran variedad de funciones que se eligen de acuerdo con el objetivo de entrenamiento de la red neuronal artificial, entre las cuales se encuentran las siguientes:

-   Identidad.

La función de identidad es una función lineal que devuelve el mismo valor de entrada como salida: $$ f(x) = x $$

-   Escalón.

La función de escalón devuelve un valor binario dependiendo de si la entrada supera un umbral ( \theta ): $$ f(x) = 
\begin{cases} 
1 & \text{si } x \geq \theta \\ 
0 & \text{si } x < \theta 
\end{cases} $$

-   Lineal a tramos.

La función lineal a tramos aplica una transformación lineal dentro de un rango específico: $$ f(x) = 
\begin{cases} 
0 & \text{si } x \leq 0 \\ 
x & \text{si } 0 < x \leq 1 \\ 
1 & \text{si } x > 1 
\end{cases} $$

-   Sigmoide. La función sigmoide suaviza la salida en un rango entre 0 y 1: $$ f(x) = \frac{1}{1 + e^{-x}} $$

-   Gaussiana. La función gaussiana calcula una salida basada en la forma de una campana, con una media (\mu) y desviación estándar (\sigma): $$ f(x) = e^{-\frac{(x - \mu)^2}{2\sigma^2}} $$

-   Sinusoidal. La función sinusoidal genera una salida oscilatoria basada en una onda sinusoidal: $$ f(x) = \sin(x) $$

**Salida:** resultado $y_i$ del procedimiento aplicado sobre los datos de entrada.

**Función de activación capa de salida:**

-   Softmax: Permite convertir un conjunto de valores en probabilidades que suman 1, su principal uso se encuentra en problemas de clasificación multiclase.

$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

-   Cross-entropy: Mide la diferencia entre las predicciones $\hat{y_i}$ y la real $y$, para el caso de clasificación lo hace de la siguiente manera:

$$
\text{Cross-Entropy Loss} = - \sum_{i=1}^{n} y_i \log(\hat{y}_i)
$$

-   Sparse Cross-Entropy Loss : Es una variante de Cross-Entropy que no requiere codificación one-hot, sino que trabaja directamente con índices de las clases verdaderas.

$$
\text{Sparse Cross-Entropy Loss} = - \log(\hat{y}_{c})
$$

-   Focal loss: Es una extensión de Cross-Entropy que aplica un factor de penalización para enfocarse más en ejemplos mal clasificados. Es especialmente útil para casos de datasets desbalanceados.

$$
\text{Focal Loss} = - \alpha_t (1 - p_t)^\gamma \log(p_t)
$$

Donde: $$
p_t = \hat{y}_c
$$

#### Tipos de redes neuronales artificiales

Teniendo en cuenta las particularidades de la arquitectura de las redes neuronales artificiales, a continuación se detallan algunos tipos diferentes de modelos, clasificados según su diseño, así como sus correspondientes aplicaciones en diferentes campos de la academia y la industria.

-   **Redes neuronales artificiales perceptrón:** tienen la arquitectura más sencilla, compuesta por nodos con una única función de activación, suelen utilizarse para tareas de clasificación binaria.

-   **Redes neuronales artificiales multicapa:** su arquitectura está conformada por capas de neuronas artificiales interconectadas y son utilizadas para tareas más complejas como toma de decisiones y clasificación multiclase.

-   **Redes neuronales artificiales convolucionales:** estas redes están especialmente diseñadas para realizar tareas relacionadas con el reconocimiento de imágenes, pues utilizan filtros convolucionales para identificar patrones y características especiales en cada pixel.

-   **Redes neuronales artificiales recurrentes:** suelen ser empleadas para tareas como reconocimiento de voz, traducción automática y generación de texto por su capacidad para procesar datos secuenciales, donde toda la información está relacionada entre sí y posee un contexto común para explicarse.

#### Aplicaciones de las redes neuronales artificiales

Algunas de las aplicaciones de las redes neuronales en la actualidad incluyen las siguientes:

-   **Reconocimiento de imágenes:** las redes neuronales pueden ser utilizadas para identificar personas y objetos en imágenes y videos.
-   **Procesamiento del lenguaje natural:** las redes neuronales pueden utilizarse en tareas de comprensión del lenguaje como la traducción automática, la generación de texto y el reconocimiento de voz.
-   **Toma de decisiones:** las redes neuronales pueden ser herramientas de ayuda para la toma de decisiones en situaciones complejas tales como el análisis financiero, que es el caso del presente ejercicio y el diagnóstico médico, entre otras.
-   **Sistemas de recomendación:** las redes neuronales pueden ser empleadas para generar recomendaciones personalizadas con base en las preferencias del usuario en diferentes plataformas de streaming, comercio electrónico y redes sociales. @profedigital

## Construcción del Modelo de Aprendizaje Automático

### Preprocesamiento

Después de haber jugado con el dataset en la sección anterior para los análisis descriptivos y exploratorios, pasamos a realizar el pre procesamiento de los datos necesario para que la arquitectura de la red neuronal sea capaz de recibirlos y además ayudar a mejorar su rendimiento. Iniciamos dividiendo el conjunto de datos en entrenamiento, validación y test para evitar cualquier tipo de data leakage durante el preprocesamiento.

#### Codificación de variables categóricas

Utilizamos tres estrategias para la clasificación de las variables categóricas según su naturaleza

1.  Ordinal Encoder: Esta técnica se aplicó cuando las variables categóricas presentaban un orden natural. La codificación numérica asignada reflejaba este orden, asegurando que los valores fueran representativos de su jerarquía inherente.

2.  Label Encoder: Se empleó en variables categóricas binarias. Este enfoque evitó imponer un orden ficticio entre los valores, lo que podría generar sesgos al interpretar una relación inexistente entre las categorías.

3.  One Hot Encoding: Diseñada para variables categóricas sin un orden natural y con más de dos niveles. Cada categoría fue representada mediante un vector binario, donde un valor de 1 indica pertenencia a una categoría específica y e asigna 0 a las demás. Se prestó especial atención al número de niveles en cada variable. En casos con muchas categorías, esta técnica podría haber generado una matriz de alta dimensionalidad, aumentando los requerimientos computacionales y complicando el entrenamiento de la red neuronal. Para evitar estos problemas, se evaluó cuidadosamente la viabilidad de aplicar este método en cada caso.

#### Imputación de datos faltantes

La técnica de imputación de datos faltantes elegidas fue Iterative Imputer para las variables numéricas debido a que muestra mejores capacidades en manejar relaciones complejas que otros métodos de imputación como lo son la media o similares, gracias a que considera todo el dataset de manera conjunta en lugar de una sola columna. Para el caso de variables catgóricas se utilizo la imputación mediante mediana.

#### Estandarización

Utilizamos **StandardScaler** debido a que normaliza las características centrando su media en 0 y escalando según la desviación estándar, lo cual es crucial en redes neuronales para garantizar estabilidad numérica y mejorar la eficiencia del entrenamiento.

Esto previene problemas como:

-   **Gradiente explosivo**: Valores extremos en las entradas producen gradientes excesivamente grandes, causando desbordamientos numéricos y actualizaciones erráticas de pesos.
-   **Gradiente desvaneciente**: Gradientes extremadamente pequeños ralentizan la convergencia, dificultando el aprendizaje efectivo.

La fórmula utilizada por **StandardScaler** es:

$$
X' = \frac{X - \mu}{\sigma}
$$

donde:

-   $X$: Valor original.
-   $u$: Media de los datos.
-   $sigma$: Desviación estándar de los datos.

Este método asegura que las características tengan una varianza de 1 y estén centradas en 0, permitiendo que funciones de activación como **sigmoid** y **tanh** operen eficientemente.

#### Balanceo de clases

El análisis descriptivo mostró que la variable respuesta del dataset está altamente desbalanceada, lo cual representa un reto significativo al crear el modelo. Un modelo entrenado en estas condiciones puede tender a predecir únicamente la clase mayoritaria, generando métricas como el **accuracy** con valores engañosamente altos, pero sin reflejar una verdadera capacidad predictiva. Para abordar este problema, utilizaremos las siguientes técnicas:

-   **Sobremuestreo y Submuestreo**: Esta estrategia combina el aumento de las clases minoritarias mediante sobremuestreo y la reducción de las clases mayoritarias mediante submuestreo. El objetivo es equilibrar la cantidad de datos entre las clases, incentivando al modelo a aprender las características de todas las clases y mejorando sus métricas al minimizar la función de pérdida. Esto fue utilizado para el modelo con todas las clases.

![Ilustración del funcionamiento de las técnicas de sobremuestreo y submuestreo. Tomado de (https://www.datasciencecentral.com/handling-imbalanced-data-sets-in-supervised-learning-using-family/)](muestreo.png)

-   **Agrupamiento de las clases**: En esta estrategia agrupamos algunas de las clases con una menor cantidad de datos, esto debido a que como se verá más adelante son clases que tienen tan pocos datos que predecirlas es altamente complejo, y que pone en evidencia además las limitaciones de técnicas de sobremuestreo y submuestreo en la presencia de clases altamente desbalanceadas.

### Diseño y Arquitectura

#### Evaluación del modelo

Dado el desbalance de la variable objetivo, utilizaremos métricas diseñadas para proporcionar una evaluación más equitativa del modelo:

-   **F1-Score Macro**: A diferencia de métricas como el **accuracy**, que tienden a favorecer la clase mayoritaria en datasets desbalanceados, el **F1-Score Macro** asigna igual importancia a todas las clases, independientemente del número de muestras. Esto nos brinda una evaluación más realista del desempeño del modelo.

#### Función de pérdida

Para abordar el desbalance de clases durante el entrenamiento, utilizaremos técnicas como:

-   **Focal Loss**: Esta función de pérdida está diseñada específicamente para problemas de clasificación desbalanceados. Su objetivo es priorizar las clases minoritarias al reducir la importancia de las predicciones correctamente clasificadas para las clases mayoritarias. Esto ayuda al modelo a concentrarse más en aprender las características de las clases menos representadas.

-   **Pesos de las Clases**: En algunos casos, asignar pesos inversamente proporcionales al tamaño de cada clase en la función de pérdida puede ser una estrategia complementaria para equilibrar la importancia de las clases en el modelo.

#### Callbacks

-   **Early Stopping**: Permite reducir los tiempos de entrenamiento al monitorear una métrica, y dado un parámetro de paciencia que indica cuantas Epoch esperar, detiene el entrenamiento si la mejora después de cumplida la paciencia no es lo suficientemente significativa para justificar seguir entrenando.
-   **Reduce Learning Rate on Plateu**: Ayuda a mantener la capacidad de mejora durante el entrenamiento, los cálculos del gradiente, que dependen de la tasa de aprendizaje pueden no ser capaces de llegar al mínimo de la función de pérdida, por lo que este callback reduce la tasa de entrenamiento si para una cantidad de Epochs dada, no ha habida una mejoría en los resultados, esto permite tener un aprendizaje con capacidad de mejoría a través de las Epochs.

#### Capas

Las capas definidas en el modelo fueron las siguiente:

##### **1. Capa de Entrada**

-   **Input(shape=(X_train_bal.shape\[1\],))**:
    -   Define el tamaño de la entrada, igual al número de características en el dataset balanceado $X_{train\_bal}$.\
    -   Es la capa inicial que recibe los datos de entrada para ser procesados por las siguientes capas.

------------------------------------------------------------------------

##### **2. Primera Capa Oculta**

-   **Dense(128, activation='relu', kernel_regularizer=l2(1e-4))**:
    -   Contiene 128 neuronas con activación ReLU, que introduce no linealidades y permite al modelo aprender patrones complejos.\
    -   Utiliza un regularizador $L_2$ con un valor de $1 \times 10^{-4}$ para prevenir el sobreajuste al penalizar pesos grandes.\
-   **BatchNormalization()**:
    -   Normaliza la salida de la capa densa para estabilizar el entrenamiento y acelerar la convergencia.\
-   **Dropout(0.2)**:
    -   Desactiva aleatoriamente el 20% de las neuronas durante el entrenamiento para mejorar la generalización y reducir el riesgo de sobreajuste.

------------------------------------------------------------------------

##### **3. Segunda Capa Oculta**

-   **Dense(64, activation='relu', kernel_regularizer=l2(1e-4))**:
    -   Reduce el número de neuronas a 64 para aprender características más específicas.\
    -   Conserva el mismo esquema de regularización $L_2$ y activación ReLU.\
-   **BatchNormalization()**:
    -   Asegura que las distribuciones de activaciones estén centradas durante el entrenamiento.\
-   **Dropout(0.2)**:
    -   Mantiene el mismo porcentaje de desactivación que la capa anterior para seguir controlando el sobreajuste.

------------------------------------------------------------------------

##### **4. Tercera Capa Oculta**

-   **Dense(32, activation='relu', kernel_regularizer=l2(1e-4))**:
    -   Reduce aún más el número de neuronas a 32, capturando características específicas de alto nivel.\
    -   Sigue utilizando activación ReLU y regularización $L_2$.\
-   **BatchNormalization()**:
    -   Continua normalizando las activaciones para estabilizar el aprendizaje.\
-   **Dropout(0.2)**:
    -   Permite que el modelo siga generalizando bien al reducir el riesgo de sobreajuste.

------------------------------------------------------------------------

##### **5. Capa de Salida**

-   **Dense(num_classes, activation='softmax')**:
    -   Contiene $num_{classes}$ neuronas, donde cada neurona representa una clase de la variable objetivo.\
    -   Usa la activación Softmax para calcular probabilidades de pertenencia para cada clase, asegurando que la suma de las probabilidades sea igual a 1.

------------------------------------------------------------------------

#### **Justificación del Diseño**

1.  **Regularización y Generalización**:
    -   La regularización $L_2$ y Dropout en cada capa oculta ayudan a prevenir el sobreajuste, especialmente relevante dado el desbalance inicial del dataset.
2.  **Normalización y Estabilidad**:
    -   BatchNormalization estabiliza el aprendizaje al controlar las distribuciones de activaciones en cada capa, acelerando la convergencia.
3.  **Arquitectura Progresiva**:
    -   La reducción progresiva de neuronas en las capas ocultas (128 → 64 → 32) permite al modelo capturar primero características generales y luego refinarlas a características específicas.
4.  **Capacidad para Multiclase**:
    -   La capa de salida con Softmax asegura que el modelo sea adecuado para problemas de clasificación multiclase, produciendo distribuciones de probabilidad por clase.

## Conclusiones y aprendizajes a partir del modelo

#### **Reporte de clasificación**

#### **Matriz de confusión**

## Puesta en producción del modelo

Debido a la naturaleza del proyecto, tareas como lo son la monitoría del modelo se dejan como futuras mejorías, nos centraremos específicamente en dos.

### Creación de la API

Se utilizó el framework de FastAPI [@fastapi] para convertir el modelo una API que pudiera realizar predicciones pasandole la información de cada usuario. En el GitHub se encuentra el archivo con el código necesario para lograr este resultado.

### Despliegue del modelo

## Caso de uso

# Aplicación web

# Video publicitario

## Contribuciones individuales

Las contribuciones realizadas por cada uno de los integrantes del equipo en el desarrollo de los ejercicios correspondientes a **Modelos de riesgo de crédito con Redes Neuronales Artificiales** se muestran en el siguiente video.